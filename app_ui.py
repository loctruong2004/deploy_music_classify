# app_ui.py
import gradio as gr
import matplotlib.pyplot as plt
from PIL import Image
import matplotlib.cm as cm
import torch.nn.functional as F
import torch
from torchvision import models, transforms
import os
import librosa as lb
import soundfile as sf
import numpy as np
import datetime
import tempfile
import re
import random

# === NEW: for YouTube download ===
from yt_dlp import YoutubeDL

class_names =  ['bolero', 'cailuong', 'cheo', 'danca', 'nhacdo']
img_height,img_width=224,224
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def load_model(model_name):
    if model_name == "Model 5 class: bolero, cailuong, cheo, danca, nhacdo":
        classes = ['bolero', 'cailuong', 'cheo', 'danca', 'nhacdo']
        model_path = "./efficientnet_b0_5_lager.pth"
        num_classes = 5
    elif model_name == "Model 7 class: bolero, cai luong, cheo, dan ca, nhac do, thieu nhi,other":
        classes = ['bolero', 'cailuong', 'cheo', 'danca', 'nhacdo', 'other', 'thieunhi']
        model_path = "./efficientnet_b0_7_final.pth"
        num_classes = 7
    elif model_name == "Model 8 class: bolero, cai luong, chauvan, cheo, dan ca, pop ,remix, rap":
        classes = ['Pop', 'bolero', 'cailuong', 'chauvan', 'cheo', 'danca', 'rap', 'remix']
        model_path = "./train_with_8_class.pth"
        num_classes = 8
    else:
        raise ValueError("Model kh√¥ng h·ª£p l·ªá")

    model = models.efficientnet_b0(pretrained=False)
    model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, num_classes)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.to(device)
    model.eval()
    return model, classes

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225])
])

def get_fft(samples, n_fft=2048, hop_length=512):
    for index, item in samples.items():
        D = np.abs(lb.stft(item["sampling"], n_fft=n_fft, hop_length=hop_length))
        samples[index]["stft"] = D
    return samples

def get_mel_spectrogram(samples, sr=22050):
    for index, item in samples.items():
        S = lb.feature.melspectrogram(y=item["sampling"], sr=sr)
        S_db = lb.amplitude_to_db(S, ref=np.max)
        samples[index]["mel-spec-db"] = S_db
    return samples

def save_mel_spec(samples, root):
    image_paths = []
    for index, item in samples.items():
        S_db = item["mel-spec-db"]
        os.makedirs(root, exist_ok=True)
        file_name = os.path.splitext(os.path.basename(item["dir"]))[0]
        out_path = os.path.join(root, file_name + ".png")
        S_db_norm = (S_db - S_db.min()) / (S_db.max() - S_db.min())
        S_rgb = cm.viridis(S_db_norm)[:, :, :3]
        S_rgb = (S_rgb * 255).astype(np.uint8)
        im = Image.fromarray(S_rgb).resize((224, 224))
        im.save(out_path)
        image_paths.append(out_path)
    return image_paths
def _split_signal_random(
    y: np.ndarray,
    sr: int,
    segment_duration: float = 30.0,
    segments_per_file: int | None = None,
    min_segments: int = 1,
    max_segments: int = 12,
    overlap_ratio: float = 0.5,
    avoid_silence: bool = True,
    top_db: int = 35,
    seed: int | None = None,
    pad_to_full: bool = True
):
    """
    Tr·∫£ v·ªÅ danh s√°ch tuple: (seg_audio_np, start_sample, end_sample, start_sec, end_sec)
    theo ƒë√∫ng logic random/overlap/n√© im l·∫∑ng.
    """
    if seed is not None:
        random.seed(seed); np.random.seed(seed)

    seg_len = int(segment_duration * sr)
    if len(y) < max(seg_len // 3, 1):
        return []

    # 1) V√πng ·ª©ng vi√™n
    candidate_intervals = [(0, len(y))]
    if avoid_silence:
        intervals = lb.effects.split(y, top_db=top_db)
        candidate_intervals = [(int(s), int(e)) for s, e in intervals if (e - s) >= max(seg_len // 3, 1)]
        if not candidate_intervals:
            candidate_intervals = [(0, len(y))]

    # 2) Suy s·ªë ƒëo·∫°n auto
    usable_len = sum(e - s for s, e in candidate_intervals)
    if segments_per_file is None:
        if usable_len < seg_len:
            n_auto = 1
        else:
            step = max(int(seg_len * (1 - max(min(overlap_ratio, 0.95), 0.0))), 1)
            n_auto = int(np.floor((usable_len - seg_len) / step)) + 1
        n_target = int(np.clip(n_auto, min_segments, max_segments))
    else:
        n_target = max(0, segments_per_file)

    if n_target == 0:
        return []

    # 3) Ch·ªçn start times ng·∫´u nhi√™n
    starts = []
    attempts, max_attempts = 0, n_target * 80
    step = max(int(seg_len * (1 - max(min(overlap_ratio, 0.95), 0.0))), 1)

    start_grid = []
    for s, e in candidate_intervals:
        if e - s < seg_len:
            if pad_to_full and (e - s) > 0:
                start_grid.append(s)
            continue
        pos = s
        while pos <= e - seg_len:
            start_grid.append(pos)
            pos += step

    if not start_grid:
        mid_start = max(0, (len(y) - seg_len) // 2)
        start_grid = [mid_start]

    while len(starts) < n_target and attempts < max_attempts:
        attempts += 1
        st = random.choice(start_grid)
        ok = True
        if overlap_ratio <= 0.01:
            for s0 in starts:
                if not (st + seg_len <= s0 or s0 + seg_len <= st):
                    ok = False; break
        if ok:
            starts.append(st)

    if not starts:
        return []

    # 4) L·∫•y segment
    segments = []
    for st in sorted(starts)[:n_target]:
        ed = st + seg_len
        seg = y[st:ed]
        if len(seg) < seg_len and pad_to_full:
            seg = np.pad(seg, (0, seg_len - len(seg)), mode="constant")
        elif len(seg) < seg_len and not pad_to_full:
            continue
        start_sec = st / sr
        end_sec = min(ed, len(y)) / sr
        segments.append((seg, st, ed, start_sec, end_sec))

    return segments
def gradio_predict(audio_file, selected_model):
    print(f"\nüëâ Nh·∫≠n file: {audio_file}")
    model, class_names_local = load_model(selected_model)
    results = []
    try:
        y, sr = lb.load(audio_file, sr=None, mono=True)
    except Exception as e:
        return f"‚ùå L·ªói khi load file: {e}"

    # ====== C·∫•u h√¨nh c·∫Øt theo logic m·ªõi ======
    SEGMENT_DURATION = 30.0
    MIN_SEGMENTS     = 1
    MAX_SEGMENTS     = 12
    OVERLAP_RATIO    = 0   #  10% ch·ªìng l·∫•n
    AVOID_SILENCE    = True
    TOP_DB           = 35
    PAD_TO_FULL      = True
    SEED             = None

    segs = _split_signal_random(
        y=y, sr=sr,
        segment_duration=SEGMENT_DURATION,
        segments_per_file=None,         # auto theo ƒë·ªô d√†i
        min_segments=MIN_SEGMENTS,
        max_segments=MAX_SEGMENTS,
        overlap_ratio=OVERLAP_RATIO,
        avoid_silence=AVOID_SILENCE,
        top_db=TOP_DB,
        seed=SEED,
        pad_to_full=PAD_TO_FULL
    )

    if not segs:
        return "‚ö†Ô∏è File qu√° ng·∫Øn ho·∫∑c kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c ƒëo·∫°n h·ª£p l·ªá."

    base_filename = os.path.splitext(os.path.basename(audio_file))[0]
    folder_path = os.path.dirname(audio_file)
    output_folder = os.path.join(folder_path, "predict")
    os.makedirs(output_folder, exist_ok=True)

    # Ghi c√°c ƒëo·∫°n ƒë√£ c·∫Øt v√† t·∫°o mel-spec
    samples = {}
    for i, (seg, st, ed, t0, t1) in enumerate(segs, 1):
        new_filename = f"{base_filename}_part{i}.wav"
        new_path = os.path.join(output_folder, new_filename)
        sf.write(new_path, seg, sr)
        samples[i] = {"dir": new_path, "sampling": seg, "t0": t0, "t1": t1}

    samples = get_fft(samples)
    samples = get_mel_spectrogram(samples, sr)
    mel_root = os.path.join(output_folder, "mel-images")
    os.makedirs(mel_root, exist_ok=True)
    list_test = save_mel_spec(samples, mel_root)

    # Suy ƒëo√°n t·ª´ng ·∫£nh mel
    for idx, path in enumerate(list_test, 1):
        image_pil = Image.open(path).convert("RGB").resize((224, 224))
        image_tensor = transform(image_pil).unsqueeze(0).to(device)

        with torch.no_grad():
            outputs = model(image_tensor)
            probs = F.softmax(outputs, dim=1)
            conf, pred = torch.max(probs, 1)

            output_class = class_names_local[pred.item()]
            percentage = conf.item() * 100

            # D√πng time t·ª´ dict samples t∆∞∆°ng ·ª©ng
            t0 = samples[idx]["t0"]
            t1 = samples[idx]["t1"]
            start_time = str(datetime.timedelta(seconds=int(t0)))
            end_time   = str(datetime.timedelta(seconds=int(t1)))
            results.append(f"[{start_time} ‚Üí {end_time}] ‚Üí {output_class} ({percentage:.2f}%)")

    return "\n".join(results)


# === NEW: helper ƒë·ªÉ t·∫£i YouTube v√† tr√≠ch xu·∫•t WAV ===
_SANITIZE = re.compile(r'[^a-zA-Z0-9_\-\.]+')

def _safe_name(s: str) -> str:
    return _SANITIZE.sub('_', s).strip('_')
from yt_dlp import YoutubeDL
from yt_dlp.utils import DownloadError

def _yt_opts_common(out_dir: str, use_cookies: bool = False):
    opts = {
        "format": "bestaudio/best",
        "outtmpl": os.path.join(out_dir, "%(title).80s.%(ext)s"),
        "noplaylist": True,
        "quiet": True,
        "no_warnings": True,
        "retries": 10,
        "fragment_retries": 10,
        "extractor_retries": 5,
        "concurrent_fragment_downloads": 4,
        "http_headers": {
            # UA ‚Äúth·∫≠t‚Äù ƒë·ªÉ h·∫°n ch·∫ø b·ªã 403
            "User-Agent": ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                           "AppleWebKit/537.36 (KHTML, like Gecko) "
                           "Chrome/120.0.0.0 Safari/537.36"),
            "Accept-Language": "en-US,en;q=0.9",
            "Referer": "https://www.youtube.com/",
        },
        "postprocessors": [
            {
                "key": "FFmpegExtractAudio",
                "preferredcodec": "wav",
                "preferredquality": "0",
            }
        ],
        "prefer_ffmpeg": True,
        "nocheckcertificate": True,    # tr√°nh cert l·ªói l·∫∑t v·∫∑t
        "geo_bypass": True,
        "forceipv4": True,             # tr√°nh IPv6 g√¢y 403 ·ªü m·ªôt s·ªë m·∫°ng
    }
    if use_cookies:
        # T·ª± l·∫•y cookie t·ª´ Chrome (Windows). N·∫øu d√πng Edge/Firefox, ƒë·ªïi l·∫°i tuple.
        # Y√™u c·∫ßu ƒëang ƒëƒÉng nh·∫≠p YT tr√™n Chrome ·ªü m√°y n√†y.
        opts["cookiesfrombrowser"] = ("chrome", )
    return opts

def download_youtube_audio(url: str, out_dir: str) -> str:
    """
    T·∫£i bestaudio t·ª´ YouTube -> WAV. Th·ª≠ 2 b∆∞·ªõc:
    1) Kh√¥ng cookie
    2) N·∫øu th·∫•t b·∫°i (403/DownloadError) -> d√πng cookies t·ª´ Chrome
    Tr·∫£ v·ªÅ ƒë∆∞·ªùng d·∫´n file WAV.
    """
    os.makedirs(out_dir, exist_ok=True)

    def _try_download(use_cookies: bool):
        ydl_opts = _yt_opts_common(out_dir, use_cookies=use_cookies)
        with YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(url, download=True)
            title = info.get("title", "audio")
            wav_guess = _safe_name(title) + ".wav"
            candidate = os.path.join(out_dir, wav_guess)
            if os.path.exists(candidate):
                return candidate
            # fallback: l·∫•y file .wav m·ªõi nh·∫•t
            wavs = [os.path.join(out_dir, f) for f in os.listdir(out_dir) if f.lower().endswith(".wav")]
            if not wavs:
                raise RuntimeError("Kh√¥ng t√¨m th·∫•y WAV sau khi tr√≠ch xu·∫•t. Ki·ªÉm tra ffmpeg/yt_dlp.")
            return max(wavs, key=os.path.getmtime)

    # Th·ª≠ kh√¥ng cookie
    try:
        return _try_download(use_cookies=False)
    except DownloadError as e:
        msg = str(e).lower()
        # N·∫øu c√≥ d·∫•u hi·ªáu 403/forbidden -> th·ª≠ l·∫°i v·ªõi cookies
        if "403" in msg or "forbidden" in msg:
            try:
                return _try_download(use_cookies=True)
            except Exception as e2:
                raise RuntimeError(f"YT 403. Th·ª≠ l·∫°i v·ªõi cookiesfrombrowser th·∫•t b·∫°i: {e2}") from e
        raise
def predict_from_ui(audio_path, model_name, input_mode, yt_url):
    try:
        if input_mode == "YouTube URL":
            if not yt_url or not yt_url.strip():
                return "‚ö†Ô∏è Vui l√≤ng nh·∫≠p YouTube URL."
            tmp_dir = os.path.join(os.getcwd(), "downloads_youtube")
            wav_path = download_youtube_audio(yt_url.strip(), tmp_dir)
            return gradio_predict(wav_path, model_name)
        else:
            if not audio_path:
                return "‚ö†Ô∏è Vui l√≤ng ch·ªçn file √¢m thanh (WAV/MP3)."
            return gradio_predict(audio_path, model_name)
    except Exception as e:
        hint = (
            "\n\nüõ†Ô∏è G·ª£i √Ω kh·∫Øc ph·ª•c:\n"
            "‚Ä¢ `pip install -U yt-dlp`\n"
            "‚Ä¢ C√†i `ffmpeg` v√† th√™m v√†o PATH\n"
            "‚Ä¢ N·∫øu v·∫´n 403: ƒëƒÉng nh·∫≠p YouTube tr√™n Chrome c√πng m√°y, r·ªìi ch·∫°y l·∫°i (app s·∫Ω t·ª± l·∫•y cookies)\n"
            "‚Ä¢ Ho·∫∑c export `cookies.txt` v√† thay b·∫±ng `cookiefile` trong yt_dlp options"
        )
        return f"‚ùå L·ªói x·ª≠ l√Ω: {e}{hint}"
# === G·ª¢I √ù NH·∫†C T∆Ø∆†NG T·ª∞ ===
def suggest_similar_songs(result_text):
    if not result_text or "‚Üí" not in result_text:
        return "‚ö†Ô∏è Vui l√≤ng ch·∫°y d·ª± ƒëo√°n tr∆∞·ªõc khi g·ª£i √Ω nh·∫°c t∆∞∆°ng t·ª±."

    # L·∫•y d√≤ng cu·ªëi c√πng v√† tr√≠ch th·ªÉ lo·∫°i d·ª± ƒëo√°n
    last_line = result_text.strip().splitlines()[-1]
    match = re.search(r"‚Üí\s*(\w+)", last_line)
    if not match:
        return "‚ö†Ô∏è Kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c th·ªÉ lo·∫°i ƒë·ªÉ g·ª£i √Ω."
    genre = match.group(1).lower()

    suggestions = {
        "bolero": ["Duy√™n Ph·∫≠n - Nh∆∞ Qu·ª≥nh", "Ai Cho T√¥i T√¨nh Y√™u - L·ªá Quy√™n",
                    "S·∫ßu T√≠m Thi·ªáp H·ªìng - Quang L√™", "N·ªói Bu·ªìn Hoa Ph∆∞·ª£ng - H∆∞∆°ng Lan",
                    "C√¥ H√†ng X√≥m - Tr∆∞·ªùng V≈©"],
        "cailuong": ["T√¨nh Anh B√°n Chi·∫øu - Minh C·∫£nh", "Lan V√† ƒêi·ªáp - Ph∆∞·ª£ng Li√™n",
                      "B√¥ng H·ªìng C√†i √Åo - Thanh Kim Hu·ªá", "Tr√°ch Ai V√¥ T√¨nh - Thanh Sang",
                      "Gi·ªçt L·ªá ƒê√†i Trang - L·ªá Th·ªßy"],
        "cheo": ["Quan √Çm Th·ªã K√≠nh", "L∆∞u B√¨nh D∆∞∆°ng L·ªÖ", "Tr∆∞∆°ng Vi√™n", "T·∫•m C√°m", "Ch·ªã D·∫≠u"],
        "danca": ["Tr·ªëng C∆°m", "L√Ω C√¢y ƒêa", "C√≤ L·∫£", "B√®o D·∫°t M√¢y Tr√¥i", "L√Ω Ng·ª±a √î"],
        "nhacdo": ["Ti·∫øn Qu√¢n Ca", "B√°c V·∫´n C√πng Ch√∫ng Ch√°u H√†nh Qu√¢n", "Nh∆∞ C√≥ B√°c H·ªì Trong Ng√†y Vui ƒê·∫°i Th·∫Øng",
                   "Chi·ªÅu Tr√™n B·∫£n Th∆∞·ª£ng", "C√¥ G√°i M·ªü ƒê∆∞·ªùng"],
        "pop": ["Em C·ªßa Ng√†y H√¥m Qua - S∆°n T√πng M-TP", "N∆°i N√†y C√≥ Anh - S∆°n T√πng M-TP",
                "Ph√≠a Sau M·ªôt C√¥ G√°i - Soobin Ho√†ng S∆°n", "Th√°ng T∆∞ L√† L·ªùi N√≥i D·ªëi C·ªßa Em - H√† Anh Tu·∫•n",
                "H∆°n C·∫£ Y√™u - ƒê·ª©c Ph√∫c"],
        "remix": ["ƒê·∫øm Sao Remix - H·ªì Quang Hi·∫øu", "H·∫° C√≤n V∆∞∆°ng N·∫Øng Remix", 
                  "Ch·ªù Ng√†y M∆∞a Tan Remix", "Bay Tr√™n ƒê∆∞·ªùng Bay Remix", "Ng√†y ·∫§y B·∫°n V√† T√¥i Remix"],
        "thieunhi": ["B√© B√© B·ªìng B√¥ng", "Con C√≤ B√© B√©", "M·ªôt Con V·ªãt", "B·∫Øc Kim Thang", "Em ƒêi Gi·ªØa Bi·ªÉn V√†ng"],
        "other": ["G√°nh H√†ng Rong", "Tr·ªü V·ªÅ D√≤ng S√¥ng Tu·ªïi Th∆°", "N·ªói Nh·ªõ M√πa ƒê√¥ng", "Chi·∫øc L√° Cu·ªëi C√πng", "T√¨nh Kh√∫c V√†ng"]
    }

    top_songs = suggestions.get(genre, [])
    if not top_songs:
        return f"üéµ Ch∆∞a c√≥ g·ª£i √Ω cho th·ªÉ lo·∫°i `{genre}`."

    out = f"### üé∂ G·ª£i √Ω nh·∫°c t∆∞∆°ng t·ª± ({genre.capitalize()})\n"
    out += "\n".join([f"- {s}" for s in top_songs])
    return out


def build_app_ui():
    with gr.Blocks(title="üîä D·ª± ƒëo√°n th·ªÉ lo·∫°i nh·∫°c t·ª´ file √¢m thanh") as app:
        gr.Markdown("## üéß Ph√¢n t√≠ch & d·ª± ƒëo√°n 30s/ƒëo·∫°n")

        with gr.Row():
            input_mode = gr.Radio(
                choices=["T·∫£i file", "YouTube URL"],
                value="T·∫£i file",
                label="Ngu·ªìn d·ªØ li·ªáu"
            )

        with gr.Row():
            audio_in = gr.Audio(type="filepath", label="üéµ Ch·ªçn file √¢m thanh (WAV, MP3)", visible=True)
            yt_url_in = gr.Textbox(label="üîó YouTube URL", placeholder="https://www.youtube.com/watch?v=...", visible=False)

        with gr.Row():
            model_in = gr.Dropdown(
                choices=[
                    "Model 8 class: bolero, cai luong, chauvan, cheo, dan ca, pop ,remix, thieu nhi",
                    "Model 5 class: bolero, cailuong, cheo, danca, nhacdo",
                    "Model 7 class: bolero, cai luong, cheo, dan ca, nhac do, thieu nhi,other",
                ],
                value="Model 8 class: bolero, cai luong, chauvan, cheo, dan ca, pop ,remix, thieu nhi",
                label="üß† Ch·ªçn m√¥ h√¨nh"
            )

        run_btn = gr.Button("Ph√¢n t√≠ch", variant="primary")
        result_out = gr.Textbox(lines=15, label="K·∫øt qu·∫£ d·ª± ƒëo√°n theo t·ª´ng ƒëo·∫°n 30s")

        suggest_btn = gr.Button("üé∂ G·ª£i √Ω nh·∫°c t∆∞∆°ng t·ª±")
        suggest_out = gr.Markdown(label="G·ª£i √Ω b√†i h√°t t∆∞∆°ng t·ª±")

        # Toggle input
        def _toggle(mode):
            return (
                gr.update(visible=(mode == "T·∫£i file")),
                gr.update(visible=(mode == "YouTube URL"))
            )

        input_mode.change(_toggle, [input_mode], [audio_in, yt_url_in])

        run_btn.click(
            predict_from_ui,
            inputs=[audio_in, model_in, input_mode, yt_url_in],
            outputs=[result_out]
        )

        suggest_btn.click(suggest_similar_songs, inputs=[result_out], outputs=[suggest_out])

    return app
